# YouTube Sentiment Analysis

This service provides a multi-step pipeline to fetch YouTube video data, analyze it with AI, and ingest the results into BigQuery.

## Features

*   **YouTube Data Fetcher**: Fetches video details and comments from a given YouTube video URL.
*   **Gemini AI Analyzer**: Analyzes the fetched data using Google's Gemini AI to generate a comprehensive sentiment analysis report.
*   **BigQuery Ingestor**: Ingests the raw and analyzed data into BigQuery for storage and further analysis.
*   **Web UI**: A simple web interface to trigger the analysis pipeline.
*   **Looker Studio Integration**: Visualize the analyzed data in Looker Studio.

## Architecture

The service is designed to run on Google Cloud Run as a containerized application. The architecture consists of the following components:

1.  **Cloud Run Service**: The main application that orchestrates the entire pipeline.
2.  **Google Cloud Storage (GCS)**: Used to store intermediate JSON files generated by the YouTube data fetcher.
3.  **Google BigQuery**: The data warehouse used to store the raw and analyzed data. The database schema is defined in `schemas.sql`.
4.  **Google Vertex AI (Gemini)**: The AI platform used to perform the sentiment analysis.
5.  **Looker Studio**: Used to visualize the analyzed data.

The pipeline works as follows:

1.  The user provides a YouTube video URL through the web UI.
2.  The Cloud Run service fetches the video details and comments and stores them as JSON files in a GCS bucket.
3.  The service then calls the Gemini AI API to analyze the data from the JSON files.
4.  The analyzed data is then ingested into BigQuery.
5.  The user can then visualize the data in Looker Studio.

## Prerequisites

Before you begin, ensure you have the following tools and accounts:

*   **Google Cloud Project**: A Google Cloud project with billing enabled.
*   **gcloud CLI**: The Google Cloud command-line tool, authenticated with your Google account.
*   **Docker**: The Docker platform for building and running containers.
*   **Go**: The Go programming language.

## Configuration

### Local Development

For local development, you need to set the following environment variables. A template is provided in the `env.yaml` file.

```bash
export YOUTUBE_API_KEY="your_youtube_api_key"
export GEMINI_API_KEY="your_gemini_api_key"
export GCS_BUCKET_NAME="your_gcs_bucket_name"
export GCP_PROJECT="your_gcp_project_id"
export GCP_LOCATION="us-central1"
export BQ_DATASET="yt_sentiment_data"
export GEMINI_MODEL="gemini-2..5-pro"
export MAX_COMMENTS_TO_FETCH="5000"
export PORT="8080"
```

Load these variables into your shell session by running:

```bash
source env.yaml
```

### Deployment to Google Cloud

For deploying to Google Cloud, you will use the following variables in the deployment scripts.

```bash
export GCP_PROJECT="your-unique-project"
export GCP_PROJECT_NUMBER="your-project-number"
export GCP_LOCATION="us-central1"       # Or your preferred region
export AR_REPO_NAME="yt-sentiment-repo"
export SERVICE_NAME="yt-sentiment"
export GCS_BUCKET_NAME="your-unique-yt-sentiment-bucket" # <-- CHANGE THIS
export BQ_DATASET="yt_sentiment_data"
export SERVICE_ACCOUNT_NAME="yt-sentiment-sa"
export PROJECT_NUMBER=$(gcloud projects describe $GCP_PROJECT --format="value(projectNumber)")
export YOUTUBE_API_KEY="your-youtube-api-key" # <-- PASTE YOUR KEY HERE
export GEMINI_API_KEY="your-gemini-api-key"   # <-- PASTE YOUR KEY HERE
```

## Deployment

Follow these steps to deploy the service to Google Cloud Run.

### Step 1: Enable APIs

Enable all necessary Google Cloud services.

```bash
gcloud services enable \
    run.googleapis.com \
    cloudbuild.googleapis.com \
    artifactregistry.googleapis.com \
    iam.googleapis.com \
    bigquery.googleapis.com \
    storage-component.googleapis.com \
    aiplatform.googleapis.com \
    generativelanguage.googleapis.com \
    youtube.googleapis.com \
    iap.googleapis.com --project=${GCP_PROJECT}
```

### Step 2: Create Service Account and Grant Permissions

1.  **Create Service Account**: Create a dedicated identity for your Cloud Run service.

    ```bash
    gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME \
        --display-name="YouTube Sentiment Analysis Service Account"
    ```

2.  **Grant Permissions**: Grant the service account permissions to access GCS, BigQuery, Vertex AI (for Gemini), and Secret Manager.

    ```bash
    # GCS: To read/write JSON files
    gsutil iam ch serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com:objectAdmin gs://${GCS_BUCKET_NAME}

    # Artifact Repo
    gcloud projects add-iam-policy-binding $GCP_PROJECT --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com" --role="roles/artifactregistry.admin"

    # Allow logging
    gcloud projects add-iam-policy-binding $GCP_PROJECT --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com" --role="roles/logging.logWriter"

    # GCS: Admin to handle buckets
    gcloud projects add-iam-policy-binding $GCP_PROJECT --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com" --role="roles/storage.admin"

    # BigQuery: To write data and run ingestion jobs
    gcloud projects add-iam-policy-binding $GCP_PROJECT --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com" --role="roles/bigquery.dataEditor"
gcloud projects add-iam-policy-binding $GCP_PROJECT --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com" --role="roles/bigquery.jobUser"

    # Vertex AI: To call the Gemini model
    gcloud projects add-iam-policy-binding $GCP_PROJECT --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com" --role="roles/aiplatform.user"
    ```

### Step 3: Create GCS and BigQuery Resources

1.  **Create GCS Bucket**: This bucket will store the intermediate JSON files.

    ```bash
    gsutil mb -l $GCP_LOCATION gs://$GCS_BUCKET_NAME
    ```

2.  **Create BigQuery Dataset and Tables**: This one-liner creates the dataset and then uses your `schemas.sql` file to create the necessary tables.

    ```bash
    bq mk --location=$GCP_LOCATION $BQ_DATASET && sed "s/your_dataset_name/$BQ_DATASET/g" schemas.sql | bq query --use_legacy_sql=false
    ```

### Step 4: Build and Push Docker Container

1.  **Create an Artifact Registry Repository**: This is where your container image will be stored.

    ```bash
    gcloud artifacts repositories create $AR_REPO_NAME \
        --repository-format=docker \
        --location=$GCP_LOCATION --project=${GCP_PROJECT}
    ```

2.  **Build the Container**: Use Cloud Build to build the container from your source code and push it to Artifact Registry.

    ```bash
    gcloud builds submit \
      --tag ${GCP_LOCATION}-docker.pkg.dev/${GCP_PROJECT}/${AR_REPO_NAME}/${SERVICE_NAME}:latest \
      --service-account=projects/${GCP_PROJECT}/serviceAccounts/${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com \
      --gcs-log-dir=gs://${GCS_BUCKET_NAME}/build/log --gcs-source-staging-dir=gs://${GCS_BUCKET_NAME}/build --region=$GCP_LOCATION
    ```

### Step 5: Deploy to Cloud Run

Deploy the container image, connecting it to the service account and all environment variables.

```bash
gcloud run deploy $SERVICE_NAME \
    --image ${GCP_LOCATION}-docker.pkg.dev/${GCP_PROJECT}/${AR_REPO_NAME}/${SERVICE_NAME}:latest \
    --platform managed \
    --region $GCP_LOCATION \
    --allow-unauthenticated \
    --execution-environment=gen2 \
    --service-account ${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com \
    --set-env-vars="YOUTUBE_API_KEY=$YOUTUBE_API_KEY,GEMINI_API_KEY=$GEMINI_API_KEY,GCS_BUCKET_NAME=$GCS_BUCKET_NAME,GCP_PROJECT=$GCP_PROJECT,GCP_LOCATION=$GCP_LOCATION,BQ_DATASET=$BQ_DATASET,GEMINI_MODEL=gemini-2..5-pro,MAX_COMMENTS_TO_FETCH=5000,PORT=8080" \
    --cpu=1 \
    --memory=1Gi \
    --concurrency=80 \
    --timeout=3600
```

## Usage

### Access the UI

Once deployed, you can access the web interface to easily run the analysis pipeline.

1.  Find your service URL in the Cloud Run console or from the output of the deploy command.
2.  Navigate to `https://<your-service-url>/ui` in your web browser.

### Visualize in Looker Studio (Optional)

After ingesting data, you can build a dashboard to visualize the AI-driven analysis.

1.  Go to Looker Studio and create a new **Blank Report**.
2.  When prompted to add data, select the **BigQuery** connector.
3.  Navigate to **My Projects** > `[Your Project]` > `yt_sentiment_data` > `analyzed` and click **Add**.
4.  You can now build charts using the fields from the `analyzed` table. For example:
    *   A **Pie Chart** with `audienceAnalysis.sentimentLabel` as the dimension.
    *   A **Table** showing `keyThemes.themeTitle` and `keyThemes.summary`.
    *   **Scorecards** for `performanceMetrics.videoStatistics.viewCount`.

## Local Development

To run the service locally, you need to have Go installed.

1.  **Set Environment Variables**: Set the environment variables as described in the `env.yaml` file.
2.  **Run the Application**:

    ```bash
    go run main.go
    ```

3.  **Access the UI**: The UI will be available at `http://localhost:8080/ui` (or a different port if you changed the `PORT` environment variable).

## Project Structure

The project follows a package-by-domain structure.

*   `main.go`: The main entry point of the application.
*   `pkgs/`: Contains the different packages of the application.
    *   `bq_ingest/ingestor.go`: Handles the ingestion of data into BigQuery.
    *   `gemini_magic/analyzer.go`: Interacts with the Gemini AI API.
    *   `models/models.go`: Contains the data models.
    *   `shared/`: Contains shared utility functions.
    *   `ui_handler/handler.go`: Handles the web UI.
    *   `yt_video/fetcher.go`: Fetches data from the YouTube API.
*   `web/`: Contains the HTML templates for the web UI.
*   `schemas.sql`: The SQL schema for the BigQuery tables.
*   `Dockerfile`: The Dockerfile for building the container image.
*   `README.md`: This file.

## Contributing

Contributions are welcome! Please open an issue or submit a pull request.

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.
