# BigQuery Ingestor

## Overview

This is the final step in the data pipeline. The service reads both the original raw data file (`<trackingId>.json`) and the AI-generated analysis file (`<trackingId>_analyzed.json`) from Google Cloud Storage. It then ingests this information into three separate tables in a BigQuery dataset: `videos`, `comments`, and `analyzed`.

The endpoint is designed to be idempotent, meaning it will not insert duplicate data if it is run multiple times with the same `trackingId`.

## How to Use

The service is triggered by making a GET request to the `/ingest` endpoint.

*   **Endpoint**: `GET /ingest`
*   **Query Parameters**:
    *   `trackingId` (required): The UUID corresponding to the data files in GCS that were generated by the `/youtube` and `/magic` endpoints.
*   **Example**:
    ```bash
    curl "http://localhost:8080/ingest?trackingId=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
    ```

## Key Functions

*   **`bq_ingest.IngestData`**: This is the HTTP handler factory that coordinates the entire ingestion process. It fetches the relevant files from GCS, unmarshals them, and uses the BigQuery client to stream the data into the appropriate tables.

*   **`bq_ingest.recordExists`**: This crucial function is what makes the endpoint idempotent. Before attempting to insert any data, it runs a `SELECT COUNT(1)` query against the target BigQuery table to check if a record with the given `trackingId` already exists. If the count is greater than zero, the insertion for that dataset is skipped, preventing duplicate entries.
